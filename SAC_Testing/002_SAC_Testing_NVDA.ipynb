{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 交易环境搭建完成！\n"
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ✅ 1️⃣ 加载股票数据，并进行预处理\n",
    "df = pd.read_csv(\"/home/jesse/Projects/RL_Testing/SAC_Testing/TandT/nvidia_stock_with_indicators.csv\",\n",
    "                 index_col=\"Date\", parse_dates=True)\n",
    "\n",
    "# ⚠️ 数据预处理：归一化（防止梯度爆炸）\n",
    "scaler = StandardScaler()\n",
    "df[df.columns] = scaler.fit_transform(df[df.columns])  \n",
    "\n",
    "# ✅ 2️⃣ 定义交易环境\n",
    "class StockTradingEnv(gym.Env):\n",
    "    def __init__(self, df, window_size=10, initial_balance=10000, transaction_cost=0.001):\n",
    "        super(StockTradingEnv, self).__init__()\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.initial_balance = initial_balance\n",
    "        self.transaction_cost = transaction_cost  # 交易成本\n",
    "        self.current_step = window_size\n",
    "\n",
    "        # 账户变量\n",
    "        self.balance = initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.total_asset_value = initial_balance\n",
    "\n",
    "        # 观测空间\n",
    "        self.feature_columns = [col for col in df.columns if col != \"Date\"]\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-np.inf, high=np.inf, shape=(window_size, len(self.feature_columns)), dtype=np.float32\n",
    "        )\n",
    "\n",
    "        # 动作空间：-1 (卖出) 到 1 (买入)\n",
    "        self.action_space = gym.spaces.Box(low=-1, high=1, shape=(1,), dtype=np.float32)\n",
    "\n",
    "    def _next_observation(self):\n",
    "        return self.df.iloc[self.current_step - self.window_size:self.current_step][self.feature_columns].values.astype(np.float32)\n",
    "\n",
    "    def step(self, action):\n",
    "        prev_asset_value = self.total_asset_value\n",
    "        current_price = self.df.iloc[self.current_step][\"Close\"]\n",
    "\n",
    "        # 计算交易量，考虑交易成本\n",
    "        trade_amount = action[0] * self.balance  \n",
    "        if trade_amount > 0:  # 买\n",
    "            shares_bought = trade_amount / current_price\n",
    "            self.shares_held += shares_bought\n",
    "            self.balance -= trade_amount\n",
    "        elif trade_amount < 0:  # 卖\n",
    "            shares_sold = min(abs(trade_amount) / current_price, self.shares_held)\n",
    "            self.shares_held -= shares_sold\n",
    "            self.balance += shares_sold * current_price\n",
    "\n",
    "        # 更新总资产\n",
    "        self.total_asset_value = self.balance + (self.shares_held * current_price)\n",
    "        \n",
    "        # 计算奖励（收益率 - 交易成本）\n",
    "        reward = (self.total_asset_value - prev_asset_value) / prev_asset_value\n",
    "        cost = self.transaction_cost * abs(action[0])  # 交易成本与交易量成正比\n",
    "        reward -= cost # I need to work on this.\n",
    "        ####### the reward is going to always be negative because it is profit rate instead of profit.\n",
    "\n",
    "        self.current_step += 1\n",
    "        terminated = self.current_step >= len(self.df) - 1\n",
    "        truncated = False  \n",
    "\n",
    "        return self._next_observation(), reward, terminated, truncated, {}\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = self.window_size\n",
    "        self.balance = self.initial_balance\n",
    "        self.shares_held = 0\n",
    "        self.total_asset_value = self.initial_balance\n",
    "        return self._next_observation(), {}\n",
    "\n",
    "    def render(self):\n",
    "        print(f\"Step: {self.current_step}, Balance: {self.balance}, Portfolio Value: {self.total_asset_value}\")\n",
    "\n",
    "# ✅ 3️⃣ 创建多进程训练环境（SAC 可支持并行环境）\n",
    "def create_env():\n",
    "    return StockTradingEnv(df)  \n",
    "\n",
    "env = make_vec_env(create_env, n_envs=1)\n",
    "\n",
    "print(\"✅ 交易环境搭建完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n",
      "Logging to ./sac_logs/SAC_7\n",
      "🎉 训练完成！模型已保存为 'sac_NVDA' 🎉\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3 import SAC\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "import pandas as pd\n",
    "\n",
    "# ✅ 4️⃣ 训练 SAC 代理\n",
    "policy_kwargs = dict(net_arch=[512, 512])  # 更大的网络结构\n",
    "model = SAC(\n",
    "    \"MlpPolicy\",\n",
    "    env,\n",
    "    verbose=1,\n",
    "    learning_rate=1e-6,  # 降低学习率，防止梯度爆炸\n",
    "    ent_coef=0.01,  # 限制温度系数，避免过度探索\n",
    "    policy_kwargs=policy_kwargs,\n",
    "    tensorboard_log=\"./sac_logs/\"\n",
    ")\n",
    "\n",
    "# ✅ 5️⃣ 训练 100 万步\n",
    "model.learn(total_timesteps=100)\n",
    "\n",
    "# ✅ 6️⃣ 保存模型\n",
    "model.save(\"sac_NVDA\")\n",
    "\n",
    "# ✅ 7️⃣ 训练完成后评估\n",
    "print(\"🎉 训练完成！模型已保存为 'sac_NVDA' 🎉\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_df = pd.read_csv(\"/home/jesse/Projects/RL_Testing/SAC_Testing/TandT//microsoft_stock_with_indicators.csv\")\n",
    "\n",
    "env = StockTradingEnv(aapl_df)\n",
    "obs, _ = env.reset()  # `gymnasium` 的 `reset` 现在返回 (obs, info)\n",
    "\n",
    "for _ in range(len(aapl_df)):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)  # `gymnasium` 需要接收 5 个返回值\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aapl_df = pd.read_csv(\"/home/jesse/Projects/RL_Testing/SAC_Testing/TandT//apple_stock_with_indicators.csv\")\n",
    "\n",
    "env = StockTradingEnv(aapl_df)\n",
    "obs, _ = env.reset()  # `gymnasium` 的 `reset` 现在返回 (obs, info)\n",
    "\n",
    "for _ in range(len(aapl_df)):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)  # `gymnasium` 需要接收 5 个返回值\n",
    "    env.render()\n",
    "    if terminated or truncated:\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 加载训练好的模型\n",
    "model = SAC.load(\"sac_NVDA\")\n",
    "\n",
    "# 运行测试\n",
    "obs, _ = env.reset()\n",
    "done = False\n",
    "total_reward = 0\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)  # 采用训练好的策略\n",
    "    obs, reward, terminated, truncated, _ = env.step(action)\n",
    "    done = terminated or truncated\n",
    "    total_reward += reward\n",
    "\n",
    "print(f\"最终收益: {total_reward}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
